#!/usr/bin/env python3
"""
ç»¼åˆåŸºå‡†æµ‹è¯•è„šæœ¬
æ¯”è¾ƒå¤šä¸ªLVI-SAMæ”¹è¿›ç‰ˆæœ¬çš„æ€§èƒ½
"""

import os
import sys
import json
import subprocess
import time
from pathlib import Path
from typing import List, Dict
import argparse


class BenchmarkSuite:
    """
    ç»¼åˆåŸºå‡†æµ‹è¯•å¥—ä»¶
    """
    
    def __init__(
        self,
        base_dir: str = "/home/cx/lvi-sam",
        output_dir: str = "experiments/evaluation"
    ):
        """
        åˆå§‹åŒ–åŸºå‡†æµ‹è¯•å¥—ä»¶
        
        Args:
            base_dir: LVI-SAMé¡¹ç›®æ ¹ç›®å½•
            output_dir: è¾“å‡ºç›®å½•
        """
        self.base_dir = Path(base_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.results = {}
        self.timestamps = {}
    
    def run_benchmark(
        self,
        method_name: str,
        launch_file: str,
        bag_file: str,
        duration: int = None
    ) -> Dict:
        """
        è¿è¡Œå•ä¸ªåŸºå‡†æµ‹è¯•
        
        Args:
            method_name: æ–¹æ³•åç§°
            launch_file: ROSå¯åŠ¨æ–‡ä»¶
            bag_file: æ•°æ®é›†bagæ–‡ä»¶
            duration: è¿è¡Œæ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œè‹¥ä¸ºNoneåˆ™è¿è¡Œæ•´ä¸ªbag
            
        Returns:
            æµ‹è¯•ç»“æœå­—å…¸
        """
        print(f"\n{'='*60}")
        print(f"æ­£åœ¨æµ‹è¯•: {method_name}")
        print(f"å¯åŠ¨æ–‡ä»¶: {launch_file}")
        print(f"æ•°æ®é›†: {bag_file}")
        print(f"{'='*60}\n")
        
        start_time = time.time()
        
        try:
            # 1. å¯åŠ¨ROSèŠ‚ç‚¹
            print("ğŸ“ Step 1: å¯åŠ¨ROSèŠ‚ç‚¹...")
            launch_cmd = [
                "roslaunch",
                launch_file,
                f"method:={method_name}"
            ]
            
            # 2. æ’­æ”¾æ•°æ®é›†
            print("ğŸ“ Step 2: æ’­æ”¾æ•°æ®é›†...")
            if duration:
                bag_cmd = ["rosbag", "play", bag_file, "-d", "3", "--duration", str(duration)]
            else:
                bag_cmd = ["rosbag", "play", bag_file, "-d", "3"]
            
            # 3. è¿è¡ŒSLAM
            print("ğŸ“ Step 3: è¿è¡ŒSLAMç³»ç»Ÿ...")
            
            # è·å–è¾“å‡ºç›®å½•
            result_dir = self.output_dir / method_name
            result_dir.mkdir(parents=True, exist_ok=True)
            
            # 4. è¯„ä¼°è½¨è¿¹
            print("ğŸ“ Step 4: è¯„ä¼°è½¨è¿¹...")
            
            # è¿™é‡Œåº”è¯¥è°ƒç”¨evaluate_trajectory.py
            
            elapsed_time = time.time() - start_time
            
            result = {
                'method': method_name,
                'status': 'SUCCESS',
                'elapsed_time': elapsed_time,
                'result_dir': str(result_dir)
            }
            
            print(f"\nâœ“ {method_name} æµ‹è¯•å®Œæˆ (è€—æ—¶: {elapsed_time:.2f}s)")
            
        except Exception as e:
            elapsed_time = time.time() - start_time
            print(f"\nâœ— {method_name} æµ‹è¯•å¤±è´¥: {e}")
            result = {
                'method': method_name,
                'status': 'FAILED',
                'elapsed_time': elapsed_time,
                'error': str(e)
            }
        
        self.results[method_name] = result
        return result
    
    def run_all_benchmarks(
        self,
        benchmarks: List[Dict]
    ) -> Dict:
        """
        è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•
        
        Args:
            benchmarks: åŸºå‡†æµ‹è¯•é…ç½®åˆ—è¡¨
                [
                    {
                        'name': 'baseline',
                        'launch': 'lvi_sam Husky.launch',
                        'bag': 'home_data/husky.bag',
                        'duration': 300
                    },
                    ...
                ]
            
        Returns:
            æ‰€æœ‰æµ‹è¯•ç»“æœ
        """
        print("\n" + "="*60)
        print("å¼€å§‹ç»¼åˆåŸºå‡†æµ‹è¯•")
        print("="*60)
        
        total_start = time.time()
        
        for benchmark in benchmarks:
            self.run_benchmark(
                method_name=benchmark['name'],
                launch_file=benchmark['launch'],
                bag_file=benchmark['bag'],
                duration=benchmark.get('duration')
            )
        
        total_time = time.time() - total_start
        
        print(f"\n{'='*60}")
        print(f"æ‰€æœ‰åŸºå‡†æµ‹è¯•å®Œæˆ (æ€»è€—æ—¶: {total_time:.2f}s)")
        print(f"{'='*60}\n")
        
        return self.results
    
    def generate_report(self) -> None:
        """
        ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š
        """
        report_path = self.output_dir / "benchmark_report.json"
        
        with open(report_path, 'w') as f:
            json.dump(self.results, f, indent=2)
        
        print(f"âœ“ æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        # æ‰“å°æ€»ç»“
        print(f"\n{'='*60}")
        print("åŸºå‡†æµ‹è¯•æ€»ç»“")
        print(f"{'='*60}\n")
        
        for method, result in self.results.items():
            print(f"æ–¹æ³•: {method}")
            print(f"  çŠ¶æ€: {result['status']}")
            print(f"  è€—æ—¶: {result['elapsed_time']:.2f}s")
            if 'error' in result:
                print(f"  é”™è¯¯: {result['error']}")
            print()


class ExperimentTracker:
    """
    å®éªŒè·Ÿè¸ªå™¨
    """
    
    def __init__(self, tracking_file: str = "experiments/experiment_log.json"):
        self.tracking_file = Path(tracking_file)
        self.tracking_file.parent.mkdir(parents=True, exist_ok=True)
        self.experiments = self._load_experiments()
    
    def _load_experiments(self) -> Dict:
        """åŠ è½½ç°æœ‰å®éªŒè®°å½•"""
        if self.tracking_file.exists():
            with open(self.tracking_file, 'r') as f:
                return json.load(f)
        return {}
    
    def add_experiment(
        self,
        exp_id: str,
        method: str,
        metrics: Dict,
        notes: str = ""
    ) -> None:
        """
        æ·»åŠ å®éªŒè®°å½•
        
        Args:
            exp_id: å®éªŒID
            method: æ–¹æ³•åç§°
            metrics: æ€§èƒ½æŒ‡æ ‡
            notes: å¤‡æ³¨
        """
        self.experiments[exp_id] = {
            'method': method,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'metrics': metrics,
            'notes': notes
        }
        self._save_experiments()
    
    def _save_experiments(self) -> None:
        """ä¿å­˜å®éªŒè®°å½•"""
        with open(self.tracking_file, 'w') as f:
            json.dump(self.experiments, f, indent=2)
    
    def get_best_experiment(self, metric: str) -> Dict:
        """
        è·å–æœ€ä½³å®éªŒ
        
        Args:
            metric: è¯„ä¼°æŒ‡æ ‡åç§°
            
        Returns:
            æœ€ä½³å®éªŒè®°å½•
        """
        best = None
        best_value = float('inf')
        
        for exp_id, exp_data in self.experiments.items():
            if metric in exp_data['metrics']:
                value = exp_data['metrics'][metric]
                if value < best_value:
                    best_value = value
                    best = exp_data
        
        return best
    
    def print_summary(self) -> None:
        """æ‰“å°å®éªŒæ€»ç»“"""
        print(f"\n{'='*60}")
        print("å®éªŒæ€»ç»“")
        print(f"{'='*60}\n")
        
        print(f"æ€»å®éªŒæ•°: {len(self.experiments)}\n")
        
        for exp_id, exp_data in sorted(self.experiments.items()):
            print(f"å®éªŒ: {exp_id}")
            print(f"  æ–¹æ³•: {exp_data['method']}")
            print(f"  æ—¶é—´: {exp_data['timestamp']}")
            print(f"  æŒ‡æ ‡: {exp_data['metrics']}")
            if exp_data['notes']:
                print(f"  å¤‡æ³¨: {exp_data['notes']}")
            print()


def main():
    parser = argparse.ArgumentParser(description='LVI-SAM ç»¼åˆåŸºå‡†æµ‹è¯•')
    parser.add_argument('--baseline', action='store_true', help='è¿è¡ŒåŸºçº¿æµ‹è¯•')
    parser.add_argument('--all', action='store_true', help='è¿è¡Œæ‰€æœ‰æµ‹è¯•')
    parser.add_argument('--method', type=str, help='æŒ‡å®šæµ‹è¯•çš„æ–¹æ³•')
    parser.add_argument('--output', type=str, default='experiments/evaluation', help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # å®šä¹‰åŸºå‡†æµ‹è¯•
    benchmarks = [
        {
            'name': 'baseline_lvi_sam',
            'launch': 'lvi_sam Husky.launch',
            'bag': 'home_data/husky.bag',
            'duration': 300
        },
        # å¯æ·»åŠ æ›´å¤šæ”¹è¿›ç‰ˆæœ¬çš„æµ‹è¯•é…ç½®
    ]
    
    suite = BenchmarkSuite(output_dir=args.output)
    
    if args.baseline:
        suite.run_all_benchmarks(benchmarks[:1])
    elif args.all:
        suite.run_all_benchmarks(benchmarks)
    elif args.method:
        for benchmark in benchmarks:
            if benchmark['name'] == args.method:
                suite.run_benchmark(
                    method_name=benchmark['name'],
                    launch_file=benchmark['launch'],
                    bag_file=benchmark['bag'],
                    duration=benchmark.get('duration')
                )
                break
    
    suite.generate_report()


if __name__ == "__main__":
    main()
