#!/usr/bin/env python3
"""
æ€§èƒ½å¯¹æ¯”åˆ†æå·¥å…·
å¯¹æ¯”baselineå’Œæ”¹è¿›ç‰ˆæœ¬çš„æ€§èƒ½æŒ‡æ ‡
"""

import json
import numpy as np
from pathlib import Path
from typing import Dict, List
import matplotlib.pyplot as plt


def load_metrics(json_path: str) -> Dict:
    """åŠ è½½è¯„ä¼°æŒ‡æ ‡"""
    try:
        with open(json_path, 'r') as f:
            return json.load(f)
    except:
        return {}


def compare_metrics(baseline_metrics: Dict, improved_metrics: Dict) -> Dict:
    """
    å¯¹æ¯”ä¸¤ä¸ªç‰ˆæœ¬çš„æŒ‡æ ‡
    
    Args:
        baseline_metrics: åŸºçº¿ç‰ˆæœ¬æŒ‡æ ‡
        improved_metrics: æ”¹è¿›ç‰ˆæœ¬æŒ‡æ ‡
        
    Returns:
        å¯¹æ¯”ç»“æœ
    """
    comparison = {
        'APE': {},
        'ATE': {},
        'ARE': {},
    }
    
    # APEå¯¹æ¯”
    if 'APE_RMSE' in baseline_metrics and 'APE_RMSE' in improved_metrics:
        baseline_ape = baseline_metrics['APE_RMSE']
        improved_ape = improved_metrics['APE_RMSE']
        improvement = (baseline_ape - improved_ape) / baseline_ape * 100
        
        comparison['APE'] = {
            'baseline': baseline_ape,
            'improved': improved_ape,
            'improvement_percent': improvement,
            'improvement_meters': baseline_ape - improved_ape,
        }
    
    # ATEå¯¹æ¯”
    if 'ATE_RMSE' in baseline_metrics and 'ATE_RMSE' in improved_metrics:
        baseline_ate = baseline_metrics['ATE_RMSE']
        improved_ate = improved_metrics['ATE_RMSE']
        improvement = (baseline_ate - improved_ate) / baseline_ate * 100
        
        comparison['ATE'] = {
            'baseline': baseline_ate,
            'improved': improved_ate,
            'improvement_percent': improvement,
            'improvement_meters': baseline_ate - improved_ate,
        }
    
    # AREå¯¹æ¯”
    if 'ARE_RMSE' in baseline_metrics and 'ARE_RMSE' in improved_metrics:
        baseline_are = baseline_metrics['ARE_RMSE']
        improved_are = improved_metrics['ARE_RMSE']
        # æ—‹è½¬è¯¯å·®ç”¨åº¦æ•°è¡¨ç¤º
        improvement = (baseline_are - improved_are) / baseline_are * 100
        
        comparison['ARE'] = {
            'baseline': baseline_are,
            'improved': improved_are,
            'improvement_percent': improvement,
            'improvement_deg': baseline_are - improved_are,
        }
    
    return comparison


def print_comparison_report(comparison: Dict, method_name: str = "Enhanced VO"):
    """
    æ‰“å°å¯¹æ¯”æŠ¥å‘Š
    
    Args:
        comparison: å¯¹æ¯”ç»“æœ
        method_name: æ”¹è¿›æ–¹æ³•åç§°
    """
    print("\n" + "="*70)
    print(f"ğŸ“Š æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š: {method_name}")
    print("="*70)
    
    print(f"\nğŸ¯ APE (ç»å¯¹ä½å§¿è¯¯å·®)")
    print(f"   â”‚")
    if comparison['APE']:
        ape = comparison['APE']
        baseline = ape['baseline']
        improved = ape['improved']
        improvement = ape['improvement_percent']
        
        print(f"   â”œâ”€ Baseline:      {baseline:.6f} m")
        print(f"   â”œâ”€ Improved:      {improved:.6f} m")
        print(f"   â”œâ”€ Improvement:   {improvement:+.2f}%")
        if improvement < 0:
            print(f"   â””â”€ âš ï¸  æ€§èƒ½ä¸‹é™äº† {abs(improvement):.2f}%")
        else:
            print(f"   â””â”€ âœ“ æ€§èƒ½æå‡äº† {improvement:.2f}%")
    else:
        print(f"   â””â”€ âœ— æ•°æ®ä¸å¯ç”¨")
    
    print(f"\nğŸ¯ ATE (ç»å¯¹è½¨è¿¹è¯¯å·®)")
    print(f"   â”‚")
    if comparison['ATE']:
        ate = comparison['ATE']
        baseline = ate['baseline']
        improved = ate['improved']
        improvement = ate['improvement_percent']
        
        print(f"   â”œâ”€ Baseline:      {baseline:.6f} m")
        print(f"   â”œâ”€ Improved:      {improved:.6f} m")
        print(f"   â”œâ”€ Improvement:   {improvement:+.2f}%")
        if improvement < 0:
            print(f"   â””â”€ âš ï¸  æ€§èƒ½ä¸‹é™äº† {abs(improvement):.2f}%")
        else:
            print(f"   â””â”€ âœ“ æ€§èƒ½æå‡äº† {improvement:.2f}%")
    else:
        print(f"   â””â”€ âœ— æ•°æ®ä¸å¯ç”¨")
    
    print(f"\nğŸ¯ ARE (ç»å¯¹æ—‹è½¬è¯¯å·®)")
    print(f"   â”‚")
    if comparison['ARE']:
        are = comparison['ARE']
        baseline = are['baseline']
        improved = are['improved']
        improvement = are['improvement_percent']
        
        print(f"   â”œâ”€ Baseline:      {baseline:.6f}Â°")
        print(f"   â”œâ”€ Improved:      {improved:.6f}Â°")
        print(f"   â”œâ”€ Improvement:   {improvement:+.2f}%")
        if improvement < 0:
            print(f"   â””â”€ âš ï¸  æ€§èƒ½ä¸‹é™äº† {abs(improvement):.2f}%")
        else:
            print(f"   â””â”€ âœ“ æ€§èƒ½æå‡äº† {improvement:.2f}%")
    else:
        print(f"   â””â”€ âœ— æ•°æ®ä¸å¯ç”¨")
    
    print("\n" + "="*70)


def create_comparison_chart(comparison: Dict, output_path: str = "evaluation_results/comparison.png"):
    """
    åˆ›å»ºå¯¹æ¯”æŸ±çŠ¶å›¾
    
    Args:
        comparison: å¯¹æ¯”ç»“æœ
        output_path: è¾“å‡ºå›¾è¡¨è·¯å¾„
    """
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    fig.suptitle('LVI-SAM æ€§èƒ½å¯¹æ¯”: Baseline vs Enhanced VO', fontsize=14, fontweight='bold')
    
    metrics = ['APE', 'ATE', 'ARE']
    colors_baseline = '#FF6B6B'
    colors_improved = '#4ECDC4'
    
    for idx, metric_name in enumerate(metrics):
        ax = axes[idx]
        metric = comparison.get(metric_name, {})
        
        if metric:
            baseline = metric['baseline']
            improved = metric['improved']
            
            x = np.arange(2)
            values = [baseline, improved]
            bars = ax.bar(x, values, color=[colors_baseline, colors_improved], alpha=0.8, edgecolor='black', linewidth=2)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, val in zip(bars, values):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height,
                       f'{val:.4f}',
                       ha='center', va='bottom', fontweight='bold', fontsize=11)
            
            # æ”¹è¿›ç™¾åˆ†æ¯”
            improvement = metric['improvement_percent']
            ax.text(0.5, max(values) * 0.5, f'{improvement:+.1f}%',
                   ha='center', va='center', fontsize=12, fontweight='bold',
                   bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))
            
            ax.set_xticks(x)
            ax.set_xticklabels(['Baseline', 'Enhanced'], fontsize=11, fontweight='bold')
            ax.set_ylabel('Error (m)' if metric_name != 'ARE' else 'Error (deg)', fontsize=11, fontweight='bold')
            ax.set_title(metric_name, fontsize=12, fontweight='bold')
            ax.grid(axis='y', alpha=0.3)
    
    plt.tight_layout()
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"\nâœ“ å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜: {output_path}")
    plt.close()


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="æ€§èƒ½å¯¹æ¯”åˆ†æ")
    parser.add_argument('--baseline-metrics', default='evaluation_results/baseline_metrics.json',
                       help='åŸºçº¿æŒ‡æ ‡JSONæ–‡ä»¶')
    parser.add_argument('--improved-metrics', default='evaluation_results/enhanced_vo_metrics.json',
                       help='æ”¹è¿›ç‰ˆæœ¬æŒ‡æ ‡JSONæ–‡ä»¶')
    parser.add_argument('--method', default='Enhanced VO',
                       help='æ”¹è¿›æ–¹æ³•åç§°')
    parser.add_argument('--chart', '-c', action='store_true',
                       help='ç”Ÿæˆå¯¹æ¯”å›¾è¡¨')
    
    args = parser.parse_args()
    
    # åŠ è½½æŒ‡æ ‡
    print(f"ğŸ“‚ åŠ è½½åŸºçº¿æŒ‡æ ‡: {args.baseline_metrics}")
    baseline_metrics = load_metrics(args.baseline_metrics)
    
    print(f"ğŸ“‚ åŠ è½½æ”¹è¿›æŒ‡æ ‡: {args.improved_metrics}")
    improved_metrics = load_metrics(args.improved_metrics)
    
    # å¯¹æ¯”
    comparison = compare_metrics(baseline_metrics, improved_metrics)
    
    # æ‰“å°æŠ¥å‘Š
    print_comparison_report(comparison, args.method)
    
    # ç”Ÿæˆå›¾è¡¨
    if args.chart:
        create_comparison_chart(comparison)
    
    # ä¿å­˜å¯¹æ¯”ç»“æœ
    output_json = 'evaluation_results/comparison_report.json'
    Path('evaluation_results').mkdir(parents=True, exist_ok=True)
    with open(output_json, 'w') as f:
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        comparison_serializable = {}
        for metric_name, metric_data in comparison.items():
            comparison_serializable[metric_name] = {
                k: float(v) if isinstance(v, (int, float, np.number)) else v
                for k, v in metric_data.items()
            }
        json.dump(comparison_serializable, f, indent=2)
    
    print(f"\nâœ“ å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜: {output_json}")


if __name__ == '__main__':
    main()
